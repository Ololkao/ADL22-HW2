{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hykao/miniconda3/envs/adl-hw2-no/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swag Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swag = load_dataset(\"swag\", \"regular\", cache_dir=\"./swag_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swag[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './swag/'\n",
    "swag_train = pd.read_csv(os.path.join(data_path, 'train.csv'), index_col=0)\n",
    "swag_val   = pd.read_csv(os.path.join(data_path, 'val.csv'), index_col=0)\n",
    "swag_test  = pd.read_csv(os.path.join(data_path, 'test.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train, val, test = Dataset.from_pandas(swag_train, preserve_index=False), Dataset.from_pandas(swag_val, preserve_index=False), Dataset.from_pandas(swag_test, preserve_index=False)\n",
    "datasets = DatasetDict()\n",
    "datasets['train'], datasets['val'], datasets['test'] = train, val, test\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"val\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {}\n",
    "data_files[\"train\"] = \"train.json\"\n",
    "data_files[\"valid\"] = \"valid.json\"\n",
    "extension = data_files[\"train\"].split('.')[-1]\n",
    "raw_datasets = load_dataset(extension, data_files=data_files)\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swag formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def swag_formatter():\n",
    "    folder = \"dataset/\"\n",
    "    corpus = json.load(open(f\"{folder}context.json\"))\n",
    "    train = json.load(open(f\"{folder}train.json\"))\n",
    "    valid = json.load(open(f\"{folder}valid.json\"))\n",
    "    test = json.load(open(f\"{folder}test.json\"))\n",
    "    save_keys = ['id', 'question', 'paragraphs', 'relevant']\n",
    "    ending_names = [f\"ending{i}\" for i in range(4)]\n",
    "    \n",
    "    for idx, data in enumerate(['train', 'valid', 'test']):\n",
    "        results = []\n",
    "        for element in eval(data):\n",
    "            pairs = {}\n",
    "            for key in save_keys:\n",
    "                if key == 'relevant':\n",
    "                    if idx != 2:\n",
    "                        pairs['label'] = element['paragraphs'].index(element[key])\n",
    "                    else:\n",
    "                        pairs['label'] = 0\n",
    "                elif key == 'paragraphs':\n",
    "                    for i, num in enumerate(element[key]):\n",
    "                        pairs[ending_names[i]] = corpus[num]\n",
    "                elif key == 'question':\n",
    "                    pairs['sent1'] = element[key]\n",
    "                    pairs['sent2'] = ''\n",
    "                else:\n",
    "                    pairs['video-id'] = element[key]\n",
    "            results.append(pairs)\n",
    "        json_obj = json.dumps(results, indent=2, ensure_ascii=False)\n",
    "        with open(f\"{folder}swag_{data}.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(json_obj)\n",
    "    \n",
    "swag_formatter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQuAD Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_v2 = False\n",
    "SQuAD = load_dataset(\"squad_v2\" if squad_v2 else \"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQuAD[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQuAD[\"train\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data_path = './SQuAD/'\n",
    "SQuAD_train = json.load(open(os.path.join(data_path, 'train-v1.1.json')))\n",
    "SQuAD_valid = json.load(open(os.path.join(data_path, 'dev-v1.1.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted = json.dumps(SQuAD_valid, indent=2)\n",
    "print(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQuAD_train[\"data\"][0][\"paragraphs\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQuAD formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "folder = \"dataset/\"\n",
    "corpus = json.load(open(f\"{folder}context.json\"))\n",
    "train = json.load(open(f\"{folder}train.json\"))\n",
    "valid = json.load(open(f\"{folder}valid.json\"))\n",
    "test = json.load(open(f\"{folder}test.json\"))\n",
    "\n",
    "def squad_formatter():\n",
    "    save_keys = ['id', 'question', 'context', 'answers']\n",
    "    for idx, data in enumerate(['train', 'valid', 'test']):\n",
    "        print(data)\n",
    "        results = []\n",
    "        for element in eval(data):\n",
    "            pairs = {}\n",
    "            for key in save_keys:\n",
    "                if key == 'answers':\n",
    "                    if idx != 2:\n",
    "                        new_dict = {}\n",
    "                        for k, v in element[key[:-1]].items():\n",
    "                            if k != \"text\":\n",
    "                                new_dict[\"answer_\" + k] = [v]\n",
    "                            else:\n",
    "                                new_dict[k] = [v]\n",
    "                        pairs[key] = new_dict\n",
    "                elif key == 'context':\n",
    "                    if idx != 2:\n",
    "                        pairs[key] = corpus[element['relevant']]\n",
    "                    else:\n",
    "                        pairs[key] = corpus[element['paragraphs'][-1]]\n",
    "                else:\n",
    "                    pairs[key] = element[key]\n",
    "            results.append(pairs)\n",
    "        json_obj = json.dumps(results, indent=2, ensure_ascii=False)\n",
    "        with open(f\"{folder}squad_{data}.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(json_obj)\n",
    "    \n",
    "squad_formatter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for swag evaluation\n",
    "import json\n",
    "valid = json.load(open(\"dataset/squad_valid.json\")) # validation\n",
    "test = json.load(open(\"format_test.json\")) # prediction\n",
    "\n",
    "total = len(valid)\n",
    "correct = 0\n",
    "for i, element in enumerate(test):\n",
    "    correct += element['context'] == valid[i]['context']\n",
    "print(f\"accuracy: {(correct/total*100):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for squad evaluation\n",
    "ans_dict = {}\n",
    "for element in valid:\n",
    "    ans_dict[element['id']] = element['answers']['text'][0]\n",
    "# json_obj = json.dumps(ans_dict, ensure_ascii=False, indent=2)\n",
    "# with open(\"ground_truths.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "#     file.write(json_obj)\n",
    "\n",
    "correct = 0\n",
    "total = len(ans_dict)\n",
    "pred = json.load(open(\"output/valid_qa/predict_predictions.json\"))\n",
    "for key, val in pred.items():\n",
    "    if ans_dict[key] == val:\n",
    "        correct += 1\n",
    "print(f\"exact_match: {(correct/total*100):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pds\n",
    "\n",
    "df_pred = pd.read_json(\"output/test_qa21/predict_predictions.json\", typ=\"series\").reset_index()\n",
    "df_pred.columns = ['id', 'answer']\n",
    "df_pred.to_csv(\"submit.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hykao/miniconda3/envs/adl-hw2-no/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from tqdm import tqdm\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.labels = [labels[label] for label in df['intent'].values]\n",
    "        self.text = [tokenizer(text, padding='max_length', max_length=512, truncation=True, return_tensors='pt') for text in df['text']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        return self.text[idx]\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        \n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json(\"data/intent/train.json\")\n",
    "# train.set_index(\"id\")\n",
    "valid = pd.read_json(\"data/intent/train.json\")\n",
    "# valid.set_index(\"id\")\n",
    "\n",
    "classes = len(set(train['intent'].values).union(set(valid['intent'].values)))\n",
    "k = 0\n",
    "for i in train['intent'].unique():\n",
    "    labels[i] = k\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.dense = nn.Linear(768,384)\n",
    "        self.out = nn.Linear(384,classes)\n",
    "        \n",
    "    def forward(self, inp, msk):\n",
    "        _, bert_out = self.bert(input_ids = inp, attention_mask = msk, return_dict = False)\n",
    "        dropped_bert_out = self.drop(bert_out)\n",
    "        activated_output = F.relu(self.dense(dropped_bert_out))\n",
    "        fin_output = self.out(activated_output)\n",
    "        return fin_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, train, val, learning_rate, epochs, batch_size):\n",
    "    train_loader = torch.utils.data.DataLoader(train,batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val,batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "        \n",
    "    train_loss_lst = []\n",
    "    val_loss_lst = []\n",
    "    train_acc_lst = []\n",
    "    val_acc_lst = []\n",
    "    for i in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        val_acc = 0.0\n",
    "        for train_input, train_label in tqdm(train_loader):\n",
    "            train_label = train_label.to(device)\n",
    "            mask = train_input['attention_mask'].to(device)\n",
    "            train_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "            \n",
    "            output = model(train_id, mask)\n",
    "            loss = criterion(output, train_label)\n",
    "            train_loss += loss.item()\n",
    "            train_acc += ((output.argmax(dim=1) == train_label).sum().item())\n",
    "            \n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_loss_lst.append(train_loss/len(train))\n",
    "        train_acc_lst.append(train_acc/len(train))\n",
    "        torch.save(model.state_dict(), os.path.join('./', f\"Bert+drop+tanh+relu-{i}.pth\"))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_input, val_label in val_loader:\n",
    "                val_label = val_label.to(device)\n",
    "                mask = val_input['attention_mask'].to(device)\n",
    "                val_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "                \n",
    "                output = model(val_id,mask)\n",
    "                loss = criterion(output,val_label)\n",
    "                val_loss+=loss.item()\n",
    "                val_acc += ((output.argmax(dim=1) == val_label).sum().item())\n",
    "            val_loss_lst.append(val_loss/len(val))\n",
    "            val_acc_lst.append(val_acc/len(val))\n",
    "        \n",
    "        print(f'train loss: {sum(train_loss_lst)/len(train_loss_lst)}')\n",
    "        print(f'train loss: {sum(val_loss_lst)/len(val_loss_lst)}')\n",
    "        print(f'train loss: {sum(train_acc_lst)/len(train_acc_lst)}')\n",
    "        print(f'train loss: {sum(val_acc_lst)/len(val_acc_lst)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 938/938 [07:30<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.2725789999802907\n",
      "train loss: 0.20213762574195862\n",
      "train loss: 0.22446666666666668\n",
      "train loss: 0.6737333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [07:31<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.20184959970712663\n",
      "train loss: 0.13346486353874207\n",
      "train loss: 0.5398999999999999\n",
      "train loss: 0.8223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [07:31<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1468596853852272\n",
      "train loss: 0.09484770091209148\n",
      "train loss: 0.6859333333333333\n",
      "train loss: 0.8789555555555556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [07:31<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1130140339265267\n",
      "train loss: 0.07303900706730783\n",
      "train loss: 0.7623333333333333\n",
      "train loss: 0.9077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [07:31<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.09145686451156934\n",
      "train loss: 0.05927341348374884\n",
      "train loss: 0.8089333333333333\n",
      "train loss: 0.9251733333333332\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE = 16\n",
    "LR = 2e-5\n",
    "model = BertClassifier()\n",
    "\n",
    "trainer(model, Dataset(train), Dataset(valid), LR, EPOCHS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertClassifier()\n",
    "state_dict = torch.load('Bert+drop+tanh+relu-4.pth')\n",
    "model = model.load_state_dict(state_dict)\n",
    "\n",
    "val = Dataset(valid)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size=16, shuffle=False)\n",
    "outputs = model(val_loader)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('adl-hw2-no')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ae4dee186e82717fa64738ff3ce0cbd15915412b42ae100da01388d5ddc6180"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
